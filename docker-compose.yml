version: '3.8'

services:
  editor:
    build:
      context: ./engine
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - ./.gabber:/app/.gabber
      - ./.secret:/app/.secret
    command: ["uv", "run", "src/main.py", "editor"]
    environment:
      - GABBER_SECRET_FILE=.secret
      - GABBER_REPOSITORY_DIR=.gabber
      - LIVEKIT_URL=ws://livekit:7880
      - LOCAL_LLM_HOST=local_llm
      - KITTEN_TTS_HOST=kitten_tts

  repository:
    build:
      context: ./engine
      dockerfile: Dockerfile
    ports:
      - "8001:8001"
    volumes:
      - ./.gabber:/app/.gabber
      - ./.secret:/app/.secret
    command: ["uv", "run", "src/main.py", "repository"]
    environment:
      - GABBER_SECRET_FILE=.secret
      - GABBER_REPOSITORY_DIR=.gabber
      - LIVEKIT_URL=ws://livekit:7880
      - LOCAL_LLM_HOST=local_llm
      - KITTEN_TTS_HOST=kitten_tts

  engine:
    depends_on:
      - livekit
    build:
      context: ./engine
      dockerfile: Dockerfile
    volumes:
      - ./.gabber:/app/.gabber
      - ./.secret:/app/.secret
    command: ["uv", "run", "src/main.py", "engine"]
    environment:
      - GABBER_SECRET_FILE=.secret
      - GABBER_REPOSITORY_DIR=.gabber
      - LIVEKIT_URL=ws://livekit:7880
      - LOCAL_LLM_HOST=local_llm
      - KITTEN_TTS_HOST=kitten_tts

  livekit:
    image: livekit/livekit-server:latest
    ports:
      - "7880:7880"
      - "60000-60100:60000-60100/udp"
    command: ["--dev", "--bind", "0.0.0.0"]
    environment:
      - LIVEKIT_LOG_LEVEL=INFO

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"

  kitten_tts:
    build:
      context: services/kitten-tts
      dockerfile: Dockerfile

  text-llm:
    build:
      context: ./services/local-llm
      dockerfile: Dockerfile
    container_name: local-llm
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ~/.cache/vllm:/root/.cache/vllm
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command:
      - --model
      - Qwen/Qwen2.5-VL-7B-Instruct-AWQ
      - --port
      - "80"
      - --gpu-memory-utilization
      - "0.9"
      - --max-model-len
      - "32000"
      - --limit-mm-per-prompt
      - '{"image":8,"video":8,"audio":8}'
      - --enable-auto-tool-choice
      - --tool-call-parser
      - hermes